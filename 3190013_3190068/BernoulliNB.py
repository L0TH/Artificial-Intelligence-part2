# -*- coding: utf-8 -*-
"""BernoulliNB.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kt6NEgRCLsUV00-eiIMOqkyrj9zoZJAf
"""

import tensorflow as tf
import numpy as np
from tqdm import tqdm
from sklearn.metrics import classification_report

def load_word(M=4000,N=0):
  #M = Most used words
  #N = Least used words

  if(M>0 and N>0):
    return("You cand give in both parameters")
  if(M>0):
    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=M)
  if(N>0):
    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=10000-N)


  word_index = tf.keras.datasets.imdb.get_word_index()
  index2word = dict((i + 3, word) for (word, i) in word_index.items())
  index2word[0] = '[pad]'
  index2word[1] = '[bos]'
  index2word[2] = '[oov]'
  x_train = np.array([' '.join([index2word[idx] for idx in text]) for text in x_train])
  x_test = np.array([' '.join([index2word[idx] for idx in text]) for text in x_test])

  vocabulary = list()
  for text in x_train:
    tokens = text.split()
    vocabulary.extend(tokens)

  vocabulary = set(vocabulary)
  print(len(vocabulary))



  x_train_binary = list()
  x_test_binary = list()

  for text in tqdm(x_train):
    tokens = text.split()
    binary_vector = list()
    for vocab_token in vocabulary:
      if vocab_token in tokens:
        binary_vector.append(1)
      else:
        binary_vector.append(0)
    x_train_binary.append(binary_vector)

  x_train_binary = np.array(x_train_binary)

  for text in tqdm(x_test):
    tokens = text.split()
    binary_vector = list()
    for vocab_token in vocabulary:
      if vocab_token in tokens:
        binary_vector.append(1)
      else:
        binary_vector.append(0)
    x_test_binary.append(binary_vector)

  x_test_binary = np.array(x_test_binary)

  return x_train_binary,x_test_binary,y_train,y_test

class BernoulliNB():
  def __init__(self):
    prop_list_X1C1=list()
    prop_list_X1C0=list()
    prop_list_X0C1=list()
    prop_list_X0C0=list()
  
  def calculate_prop(self,x_train_binary,y_train,X,C):
      prop_list_X1C1=list()
      sum=0
      for i in range(len(x_train_binary[0])):
        count_1=0
        for j in range(len(x_train_binary)):
          if(x_train_binary[j][i]==X and y_train[j]==C):
            count_1=count_1+1
            sum+=1
        prop_list_X1C1.append(count_1)
      for i in range(len(prop_list_X1C1)):
        prop_list_X1C1[i]=np.log((prop_list_X1C1[i]+1)/(sum+2))
      return prop_list_X1C1
  
  def fit(self,x_train_binary,y_train):
    self.prop_list_X1C1=self.calculate_prop(x_train_binary,y_train,1,1)
    self.prop_list_X1C0=self.calculate_prop(x_train_binary,y_train,1,0)
    self.prop_list_X0C1=self.calculate_prop(x_train_binary,y_train,0,1)
    self.prop_list_X0C0=self.calculate_prop(x_train_binary,y_train,0,0)

  def predict(self,x_test_binary):
    
    y_predict=list()
    count_neg=0
    count_pos=0
    for test in x_test_binary:
      prop_pos=np.log(0.5)
      prop_neg=np.log(0.5)
      for i in range(len(test)):
        if(test[i]==1):
          prop_pos+=(self.prop_list_X1C1[i])
          prop_neg+=(self.prop_list_X1C0[i])
        else:
          prop_pos+=(self.prop_list_X0C1[i])
          prop_neg+=(self.prop_list_X0C0[i])

      if ((prop_pos)>(prop_neg)):
        y_predict.append(1)
      elif((prop_neg)>(prop_pos)):
        y_predict.append(0)
      else:
        y_predict.append(0)
    return y_predict

"""
**LOAD DATA**

"""

x_train_binary,x_test_binary,y_train,y_test=load_word(2000)




def learning_curve(x_binary,y,percentage):
  x_part=x_binary[:int(len(x_binary)*percentage)]
  y_part=y[:int(len(y)*percentage)]
  nb=BernoulliNB()
  nb.fit(x_train_binary[:int(len(x_binary)*percentage)],y_train[:int(len(y)*percentage)])
  print(classification_report(y_part, nb.predict(x_part), labels=[0, 1]))

"""**TRAIN**"""

learning_curve(x_train_binary,y_train,0.1)
learning_curve(x_train_binary,y_train,0.2)
learning_curve(x_train_binary,y_train,0.3)
learning_curve(x_train_binary,y_train,0.4)
learning_curve(x_train_binary,y_train,0.5)
learning_curve(x_train_binary,y_train,0.6)
learning_curve(x_train_binary,y_train,0.7)
learning_curve(x_train_binary,y_train,0.8)
learning_curve(x_train_binary,y_train,0.9)
learning_curve(x_train_binary,y_train,1)

"""**TEST**"""

learning_curve(x_test_binary,y_test,0.1)
learning_curve(x_test_binary,y_test,0.2)
learning_curve(x_test_binary,y_test,0.3)
learning_curve(x_test_binary,y_test,0.4)
learning_curve(x_test_binary,y_test,0.5)
learning_curve(x_test_binary,y_test,0.6)
learning_curve(x_test_binary,y_test,0.7)
learning_curve(x_test_binary,y_test,0.8)
learning_curve(x_test_binary,y_test,0.9)
learning_curve(x_test_binary,y_test,1)