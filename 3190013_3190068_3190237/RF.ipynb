{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of RF.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Ilopoihsh tou Random Forest"
      ],
      "metadata": {
        "id": "thKl72K4W4gw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def load_word(M=4000,N=0):\n",
        "  #M = Most used words\n",
        "  #N = Least used words\n",
        "  \n",
        "  if(M>0 and N>0):\n",
        "    return(\"You cand give in both parameters\")\n",
        "  if(M>0):\n",
        "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=M)\n",
        "  if(N>0):\n",
        "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=10000-N)\n",
        "\n",
        "\n",
        "  word_index = tf.keras.datasets.imdb.get_word_index()\n",
        "  index2word = dict((i + 3, word) for (word, i) in word_index.items())\n",
        "  index2word[0] = '[pad]'\n",
        "  index2word[1] = '[bos]'\n",
        "  index2word[2] = '[oov]'\n",
        "  x_train = np.array([' '.join([index2word[idx] for idx in text]) for text in x_train])\n",
        "  x_test = np.array([' '.join([index2word[idx] for idx in text]) for text in x_test])\n",
        "\n",
        "  vocabulary = list()\n",
        "  for text in x_train:\n",
        "    tokens = text.split()\n",
        "    vocabulary.extend(tokens)\n",
        "\n",
        "  vocabulary = set(vocabulary)\n",
        "  print(len(vocabulary))\n",
        "\n",
        "\n",
        "\n",
        "  x_train_binary = list()\n",
        "  x_test_binary = list()\n",
        "\n",
        "  for text in tqdm(x_train):\n",
        "    tokens = text.split()\n",
        "    binary_vector = list()\n",
        "    for vocab_token in vocabulary:\n",
        "      if vocab_token in tokens:\n",
        "        binary_vector.append(1)\n",
        "      else:\n",
        "        binary_vector.append(0)\n",
        "    x_train_binary.append(binary_vector)\n",
        "\n",
        "  x_train_binary = np.array(x_train_binary)\n",
        "\n",
        "  for text in tqdm(x_test):\n",
        "    tokens = text.split()\n",
        "    binary_vector = list()\n",
        "    for vocab_token in vocabulary:\n",
        "      if vocab_token in tokens:\n",
        "        binary_vector.append(1)\n",
        "      else:\n",
        "        binary_vector.append(0)\n",
        "    x_test_binary.append(binary_vector)\n",
        "\n",
        "  x_test_binary = np.array(x_test_binary)\n",
        "\n",
        "  return x_train_binary,x_test_binary,y_train,y_test, x_train\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VD_D1J7k5IkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np \n",
        "from collections import Counter\n",
        "\n",
        "def bootstrap_sample(X, y):\n",
        "    n_samples = X.shape[0]\n",
        "    idxs = np.random.choice(n_samples, n_samples, replace=True)\n",
        "    return X[idxs], y[idxs]\n",
        "\n",
        "def vote(y):\n",
        "    counter = Counter(y)\n",
        "    most_common = counter.most_common(1)[0][0]\n",
        "    return most_common\n",
        "\n",
        "class RandomForest:\n",
        "    def __init__(self, n_trees=10):\n",
        "        self.n_trees = n_trees\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        self.trees = []\n",
        "        for _ in range(self.n_trees):\n",
        "\n",
        "            tree=ID3(50)\n",
        "            X_samp, y_samp = bootstrap_sample(X, y)\n",
        "            tree.fit(x_train_binary[:int(len(X)*0.3)],y_train[:int(len(y)*0.3)])\n",
        "            tree.fit(X_samp, y_samp)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
        "        tree_preds = np.swapaxes(tree_preds, 0, 1)\n",
        "        y_pred = [vote(tree_pred) for tree_pred in tree_preds]\n",
        "        return np.array(y_pred)\n",
        "\n",
        "\n",
        "def accuracy(y_true, y_pred):   \n",
        "    accuracy = np.sum(y_true == y_pred)/len(y_true)   \n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "UD6bTCsAv6Yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_binary,x_test_binary,y_train,y_test, x_train=load_word()"
      ],
      "metadata": {
        "id": "sGl62EIl5c9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "#mabe change the train name \n",
        "def learning_curve(x_binary,y,percentage,num_of_trees=3):\n",
        "  x_part=x_binary[:int(len(x_binary)*percentage)]\n",
        "  y_part=y[:int(len(y)*percentage)]\n",
        "  rf=RandomForest(n_trees = num_of_trees)\n",
        "  rf.fit(x_train_binary[:int(len(x_binary)*percentage)],y_train[:int(len(y)*percentage)])\n",
        "  print(classification_report(y_part, rf.predict(x_part), labels=[0, 1]))\n",
        "\n",
        "\n",
        "\n",
        "learning_curve(x_train_binary,y_train,0.1)\n",
        "learning_curve(x_train_binary,y_train,0.2)\n",
        "learning_curve(x_train_binary,y_train,0.3)\n",
        "learning_curve(x_train_binary,y_train,0.4)\n",
        "learning_curve(x_train_binary,y_train,0.5)\n",
        "learning_curve(x_train_binary,y_train,0.6)\n",
        "learning_curve(x_train_binary,y_train,0.7)\n",
        "learning_curve(x_train_binary,y_train,0.8)\n",
        "learning_curve(x_train_binary,y_train,0.9)\n",
        "learning_curve(x_train_binary,y_train,1)"
      ],
      "metadata": {
        "id": "FK2R-WuzrN7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_curve(x_test_binary,y_test,0.1)\n",
        "learning_curve(x_test_binary,y_test,0.2)\n",
        "learning_curve(x_test_binary,y_test,0.3)\n",
        "learning_curve(x_test_binary,y_test,0.4)\n",
        "learning_curve(x_test_binary,y_test,0.5)\n",
        "learning_curve(x_test_binary,y_test,0.6)\n",
        "learning_curve(x_test_binary,y_test,0.7)\n",
        "learning_curve(x_test_binary,y_test,0.8)\n",
        "learning_curve(x_test_binary,y_test,0.9)\n",
        "learning_curve(x_test_binary,y_test,1)"
      ],
      "metadata": {
        "id": "zrUHnwirvnY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ilopoihsh tou ID3:"
      ],
      "metadata": {
        "id": "OIdZM3RSSmNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Word:\n",
        "\n",
        "  def __init__(self, value:str, index:int):\n",
        "    self.value = value\n",
        "    self.index = index\n",
        "    self.ig = None\n",
        "    self.entropy0 = None\n",
        "    self.entropy1 = None\n",
        "  \n",
        "  def setIg(self, ig):\n",
        "    if(ig != None):\n",
        "      self.ig = float(ig)\n",
        "      \n",
        "  def getIg(self):\n",
        "    return self.ig\n",
        "\n",
        "  def setEntropy0(self, entropy0):\n",
        "    self.entropy0 = entropy0\n",
        "\n",
        "  def setEntropy1(self, entropy1):\n",
        "    self.entropy1  = entropy1\n",
        "\n",
        "  def __str__(self):\n",
        "    return self.value\n",
        "\n",
        "  def __repr__(self):\n",
        "    return self.value + \" \" + str(self.ig)\n",
        "  "
      ],
      "metadata": {
        "id": "bbX7fXjxSnkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "  value: Word\n",
        "  left = None # must be Node\n",
        "  right = None # must be Node\n",
        "  depth = None\n",
        "  p0 = None\n",
        "  p1 = None\n",
        "\n",
        "  def __init__(self, v: int = None, depth = None):\n",
        "    self.value = v\n",
        "    if depth == None:\n",
        "      self.depth = 10\n",
        "    else:\n",
        "      self.depth = depth\n",
        "  \n",
        "  def setLeftChild(self, v: Word):\n",
        "    self.left = v\n",
        "\n",
        "  def setRightChild(self, v: Word):\n",
        "    self.right  = v\n",
        "\n",
        "  def setChilds(self, l, r):\n",
        "    self.setLeftChild(l)\n",
        "    self.setRightChild(r)\n",
        "\n",
        "  def setP0(self, p0):\n",
        "    self.p0 = p0\n",
        "  \n",
        "  def setP1(self, p1):\n",
        "    self.p1 = p1\n",
        "\n",
        "  def __str__(self):\n",
        "    return self.value.value"
      ],
      "metadata": {
        "id": "GIMA0e4ZSpnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_entropy_word(binary_array, result_array, c, word, word_position, cProb):\n",
        "  if(cProb == 1 or cProb == 0):\n",
        "    return 0\n",
        "  return - ( cProb * np.log2(cProb) ) - ( (1.0 - cProb) * np.log2(1.0 - cProb))\n",
        "\n",
        "\n",
        "def calculate_entropy_total(binary_array, result_array):\n",
        "  first = 0\n",
        "  count = 0\n",
        "  for element in binary_array:\n",
        "      if(result_array[count] == 1):\n",
        "        first += 1\n",
        "      count += 1\n",
        "  cProb = first/binary_array.shape[0]\n",
        "  if(cProb == 1 or cProb == 0):\n",
        "    return 0      \n",
        "  return - ( cProb * np.log2(cProb) ) - ( (1.0 - cProb) * np.log2(1.0 - cProb))"
      ],
      "metadata": {
        "id": "FnCHrUVOSrar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -1 = has not found the word\n",
        "def return_word(word):\n",
        "  count = 0\n",
        "  for element in vocabulary:\n",
        "    if word == element:\n",
        "      return count\n",
        "    else:\n",
        "      count += 1\n",
        "  \n",
        "  return -1"
      ],
      "metadata": {
        "id": "cX5rod7_St1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_probability(array, result_array, c, word, word_position = -1):\n",
        "  #Entropy\n",
        "  has_the_word = 0 # c value\n",
        "  count = 0 # lengeth\n",
        "  is_positive = 0\n",
        "  if word_position == -1:\n",
        "    word_position = return_word(word) # the word column\n",
        "  \n",
        "  for element in array:\n",
        "      element_i = element[word_position]\n",
        "      count += 1\n",
        "      if(element_i == c):\n",
        "        has_the_word += 1\n",
        "        if(result_array[count-1] == 1):\n",
        "          is_positive += 1\n",
        "  \n",
        "  if(has_the_word == 0):\n",
        "    return 0\n",
        "  \n",
        "  return is_positive/has_the_word"
      ],
      "metadata": {
        "id": "961uD4czSv7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_probability_total(result_array):\n",
        "  #Node \n",
        "  count = 0\n",
        "  p = 0\n",
        "  for x in result_array:\n",
        "    if(x == 1):\n",
        "      p += 1\n",
        "    count += 1\n",
        "  return p/count"
      ],
      "metadata": {
        "id": "1QT8yvXISxnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_IG(binary_array, result_array, word, calculate_entrupy_total):\n",
        "    word.setIg(calculate_entrupy_total - calculate_sum(binary_array, result_array, word))"
      ],
      "metadata": {
        "id": "cFXj6TObSzMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hasTheWord(word1: Word, word2: Word, array):\n",
        "  index1 = int(word1.index)\n",
        "  index2 = int(word2.index)\n",
        "  for x in array:\n",
        "    if(x[index1] == 1 and x[index2] == 1):\n",
        "      return 1\n",
        "  return 0"
      ],
      "metadata": {
        "id": "XJ9iWwm2S1T5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_words(array, word_index, keep_the_word = True):\n",
        "  num = []\n",
        "  c = 1\n",
        "  if(keep_the_word):\n",
        "    c = 0\n",
        "  count = 0\n",
        "  for x in array:\n",
        "    if(x[word_index] == c):\n",
        "      num.append(count)\n",
        "    count+=1\n",
        "\n",
        "  return num"
      ],
      "metadata": {
        "id": "f-CNefB_S3R1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def findValueFromArray(array, index):\n",
        "  if len(array) < index:\n",
        "    return None\n",
        "  try:\n",
        "    if(array[index] == 1):\n",
        "     return index\n",
        "  except:\n",
        "    return None\n",
        "  return None"
      ],
      "metadata": {
        "id": "V4RwyKYYS5NQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math \n",
        "def get_my_key(obj:Word):\n",
        "  return obj.getIg()\n",
        "\n",
        "def next_word_subtree(array, result_array, words):\n",
        "  i = 0\n",
        "  et = calculate_entropy_total(array, result_array)\n",
        "  words_to_remove = list()\n",
        "  for word in words:\n",
        "   word.setEntropy0(None)\n",
        "   word.setEntropy1(None)\n",
        "   word.setIg(None)\n",
        "   calculate_IG(array, result_array, word, et)\n",
        "  for x in words:\n",
        "    if x.ig == None or math.isnan(x.ig):\n",
        "     words_to_remove.append(x)\n",
        "  for x in words_to_remove:\n",
        "      words.remove(x)\n",
        "  words = sorted(words, key=get_my_key, reverse=True)\n",
        "  re = words.pop()\n",
        "  return (re, words)\n",
        "\n"
      ],
      "metadata": {
        "id": "88mFzMh9S7Wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_words():\n",
        "  words = list()\n",
        "  i = 0\n",
        "  for x in vocabulary:\n",
        "    if(x != '[pad]' and x!= '[bos]' and x != '[oov]'):\n",
        "      words.append(Word(x, i))\n",
        "    i+= 1\n",
        "\n",
        "  return words"
      ],
      "metadata": {
        "id": "zJN5IhehS_T-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_sum(binary_array, result_array, word):\n",
        "    word_position = word.index\n",
        "    p0 = (calculate_probability(binary_array, result_array, 0, word, word_position))\n",
        "    p1 = 1 - p0\n",
        "    word.setEntropy0(calculate_entropy_word(binary_array, result_array, 0, word,  word_position, p0))\n",
        "    word.setEntropy1(calculate_entropy_word(binary_array, result_array, 1, word,  word_position, p1))\n",
        "    if p0 == 0 or word.entropy0 ==0:\n",
        "      a = 0\n",
        "    else:\n",
        "      a = np.log2(p0 * word.entropy0)\n",
        "\n",
        "    if p1 == 0 or word.entropy1 ==0:\n",
        "      b = 0\n",
        "    else:\n",
        "      b = np.log2(p1 * word.entropy1)\n",
        "    return  a + b"
      ],
      "metadata": {
        "id": "CUXmLpjsTBSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ID3:\n",
        "  def __init__(self, depth):\n",
        "    self.tree = None\n",
        "    self.depth = depth\n",
        "\n",
        "  def anadromi(self, array, result_array, depth, words = None):\n",
        "    if(words == None):\n",
        "      words = calculate_words()\n",
        "    if(len(array) == 0 or len(words) == 0 or depth == 0):\n",
        "      return None\n",
        "    data = next_word_subtree(array, result_array, words)\n",
        "    word = data[0]\n",
        "    words = data[1]\n",
        "    node = Node(word, depth)\n",
        "    node.setP1(calculate_probability_total(result_array))\n",
        "    node.setP0(1-node.p1)\n",
        "    l_num = remove_words(array, word.index, False)\n",
        "    l_array = np.delete(array, l_num, 0)\n",
        "    l_result_array = np.delete(result_array, l_num, 0)\n",
        "    node.setLeftChild(self.anadromi(l_array, l_result_array, depth-1, words.copy()))\n",
        "    r_num = remove_words(array, word.index)\n",
        "    r_array = np.delete(array, r_num, 0)\n",
        "    r_result_array = np.delete(result_array, r_num)\n",
        "    node.setRightChild(self.anadromi(r_array, r_result_array, depth-1, words.copy()))\n",
        "    return node\n",
        "\n",
        "  def fit(self, array, result_array):\n",
        "      self.tree = self.anadromi(array, result_array, self.depth)   \n",
        "\n",
        "  def predict(self, table):\n",
        "    good_result = 0\n",
        "    count = 0\n",
        "    results_array = []\n",
        "    for sentense in table:\n",
        "      tree = self.tree\n",
        "      result = None\n",
        "      while True:\n",
        "        if tree.left == None and tree.right == None or tree.p1 == 1 or tree.p0 == 1:\n",
        "          result = tree\n",
        "          break\n",
        "        else:\n",
        "          sentense_word = findValueFromArray(sentense, tree.value.index)\n",
        "          if(sentense_word == None):\n",
        "            if tree.left == None:\n",
        "              result = tree\n",
        "              break\n",
        "            tree = tree.left\n",
        "          else:\n",
        "            if tree.right == None :\n",
        "              result = tree\n",
        "              break \n",
        "            tree = tree.right\n",
        "      c = 1\n",
        "      if(result.p0 > result.p1):\n",
        "        c = 0\n",
        "      results_array.append(c)\n",
        "    return results_array"
      ],
      "metadata": {
        "id": "qaCCXakcTD3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "#mabe change the train name \n",
        "def learning_curve(x_binary,y,percentage):\n",
        "  x_part=x_binary[:int(len(x_binary)*percentage)]\n",
        "  y_part=y[:int(len(y)*percentage)]\n",
        "  nb=ID3(100)\n",
        "  nb.fit(x_train_binary[:int(len(x_binary)*percentage)],y_train[:int(len(y)*percentage)])\n",
        "  print(classification_report(y_part, nb.predict(x_part), labels=[0, 1]))"
      ],
      "metadata": {
        "id": "JwJVbeTATJYg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}